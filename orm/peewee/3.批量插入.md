最近，需要同步数据到Mysql中，数据量有几百万。但是，自己写一个for循环，然后使用`Model.create()`添加，发现这种方式特别慢。难道，像去年爬数据一样，将几百万的数据从Redis取出来，然后使用多线程进行保存？

在Google上搜索了之后，找到一种更简单的方式，那就是使用Peewee原生的方法`insert_many()`，进行批量数据插入。

那么，它的速度有多快？

下面，是我简单的比较了插入10000条数据到本地数据库中，四种方式所需要的时间。

## 第1 种, for循环和Model.create()

代码如下：



```go
from xModels import XUser, database
import time

NUM = 10000
start_time = time.time()

users = []
for i in range(NUM):
    XUser.create(phone='13847374833', password='123456')

print("插入{}条数据, 花费: {:.3}秒".format(NUM, time.time()-start_time))
```

结果：插入10000条数据, 花费: 10.5秒

## 第2 种，for循环和Model.create()，并放入事务中

代码如下：



```python
from xModels import XUser, database
import time

NUM = 10000
start_time = time.time()

with database.atomic():
    for i in range(NUM):
        XUser.create(phone='13847374833', password='123456')

print("插入{}条数据, 花费: {:.3}秒".format(NUM, time.time()-start_time))
```

结果：插入10000条数据, 花费: 4.94秒

## 第3 种，使用原生的insert_many()方法



```go
from xModels import XUser, database
import time

NUM = 10000
data = [{
            'phone': '13847374833',
            'password': '123456'
        } for i in range(NUM)]

start_time = time.time()

for i in range(0, NUM, 100):
    XUser.insert_many(data[i:i + 100]).execute()

print("插入{}条数据, 花费: {:.3}秒".format(NUM, time.time()-start_time))
```

结果：插入10000条数据, 花费: 0.505秒

## 第4 种，使用原生的insert_many()方法，并放入事务中



```python
from xModels import XUser, database
import time

NUM = 10000
data = [{
            'phone': '13847374833',
            'password': '123456'
        } for i in range(NUM)]

start_time = time.time()

with database.atomic():
    for i in range(0, NUM, 100):
        # 每次批量插入100条，分成多次插入
        XUser.insert_many(data[i:i + 100]).execute()

print("插入{}条数据, 花费: {:.3}秒".format(NUM, time.time()-start_time))
```

结果：插入10000条数据, 花费: 0.401秒

**结论**

- `insert_many()`比使用for+Model.create()方式快很多，在上面例子中快了十倍不止
- 使用事务，可以些许提升

