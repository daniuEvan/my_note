# 一. Kubernetes 工作负载

> pod(豆荚):pod是k8s集群中最小的资源单位, 所有的工作负载都是为了控制pod
>
> - doc : https://kubernetes.io/zh/docs/concepts/workloads/controllers/

![image-20211225170626710](https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211225170626710.png)

- **Deployment** 
  - **无状态应用部署**, 比如微服务, 提供多副本等功能, 应用回退或者故障迁移当前数据丢失,ip也可能会丢失
- **StatefulSet**
  - **有状态应用部署**, 比如redis,database等, 提供稳定的存储和网络功能, 回退或者故障迁移数据完全备份
- **DamonSet**
  - **守护型应用部署**, 比如日志收集组件, 每个机器都运行一份,有且只有一份
- **Job/CronJob**
  - **定时任务部署**

# 二. Kubernetes 实战

> doc : https://kubernetes.io/zh/docs/concepts/workloads/controllers/

1. 资源创建方式

   - 命令行

   - YAML (推荐)
   - 可视化界面 (推荐)

2. Namespace

   > 名称空间用来隔离资源

   - 命令行创建命名空间

     ```sh
     kubectl create ns hello
     kubectl delete ns hello
     ```

   - yaml创建命名空间

     ```yaml
     apiVersion: v1
     kind: Namespace
     metadata:
       name: hello
     ```

     `kubectl apply -f ns.yaml`

     `kubectl delete -f ns.yaml`

3. 获取dashboard token

   ```sh
   kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
   ```

   

## 1. Pod

> - 推荐使用deployment
> - Pod(豆荚): 可以类比docker 的container, pod 是在docker 容器的上层封装
> - 运行中的一组(docker)容器，Pod是kubernetes中应用的最小单位.
> -  pod 之间资源隔离, 同一个pod之间共享存储,网络资源(ip 和 端口唯一,可以理解为同一个pod就是一台虚拟机).
>
> - pod可以封装多个服务(docker 容器)
> - 每一个pod k8s都会分配一个ip   `kubectl get pod -o wide`查看

<img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/1625484036923-09a15ef3-33dc-4e29-91e4-e7fbc69070ce.png" alt="image.png" style="zoom:33%;" />

- 创建pod

  - 命令行创建

    ```sh
    kubectl run mynginx --image=nginx  # 创建pod  name: mynginx 指定镜像为nginx
    
    kubectl get pod -n default  # 查看 pod 信息 -n 指定命名空间默认是default  -A查看全部命名空间的POD
    kubectl describe pod mynginx # 查看 mynginx pod描述信息
    
    # 查看default名称空间的Pod
    kubectl get pod 
    kubectl get pod -n default 
    kubectl get pod -n default -w # 实时监控
    # pod描述信息
    kubectl describe pod Pod名字
    # 删除pod
    kubectl delete pod Pod1名字 Pod2名字 -n default  # -n 指定命名空间默认是default
    kubectl delete pod --all -n default # 清除所有的pod
    # 查看Pod的运行日志
    kubectl logs Pod名字
    
    # 每个Pod - k8s都会分配一个ip
    kubectl get pod -o wide
    # 使用Pod的ip+pod里面运行容器的端口
    curl 192.168.169.136
    # 进入pod 
    kubectl exec -it mynginx(Pod名字) -- /bin/bash
    exit  # 退出pod
    
    # 集群中的任意一个机器以及任意的应用都能通过Pod分配的ip来访问这个Pod
    
    # 实时监控
    watch -n 1 kubectl get pod  # -n 1 每隔1s刷新
    ```

  - 配置文件创建

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        run: mynginx
      name: mynginx
    #  namespace: default
    spec:
      containers:
      - image: nginx
        name: nginx
      - image: tomcat:8.5.68  # 指定多个容器
        name: tomcat
    ```

    <img src="https://cdn.nlark.com/yuque/0/2021/png/1613913/1625553938232-51976552-5bab-4c98-bb8d-c4bf612bf866.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_15%2Ctext_YXRndWlndS5jb20gIOWwmuehheiwtw%3D%3D%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10" alt="image.png" style="zoom:33%;" />

  - dashboard 创建

  

## 2. Deployment  !!!

> - 推荐使用deployment 
> - Deployment (部署): 控制Pod，使Pod拥有多副本，自愈，扩缩容等能力
> - Deployment 是无状态的资源类型, 版本回退或者扩缩容自愈等之前的数据会丢失,
>
> - 创建一个 deployment 应用部署时, 该应用部署会自动创建一个名字为 xxname-xxxx 的pod. 删除该pod的时候deployment会自动再创建一个pod, 这就是deployment的自愈能力

- Deployment常用命令

  ```sh
  # 创建 deployment
  kubectl create deployment mytomcat --image=tomcat:8.5.68 
  # 删除 deployment
  kubectl delete deployment mytomcat  # 删除deployment 后所有的资源pod都会被删除
  # 多副本创建deployment
  kubectl create deployment my-dep --image=nginx --replicas=3 # 3副本replicas
  ```

  

#### 2.1 自愈

> 创建一个 deployment 应用部署时, 该应用部署会自动创建一个名字为 xxname-xxxx 的pod. 删除该pod的时候deployment会自动再创建一个pod, 这就是deployment的自愈能力

- test 测试

  ```sh
  # 清除所有Pod，比较下面两个命令有何不同效果？
  kubectl run mynginx --image=nginx
  
  kubectl create deployment mytomcat --image=tomcat:8.5.68   # 创建一个deployment应用部署, 该应用部署会自动创建一个名字为 mytomcat-xxxx 的pod. 
  
  # 效果展示
  kubectl delete pod mynginx -n default # 删除后不再存在该pod
  kubectl delete pod mytomcat-6f5f895f4f-gdd4w -n default # 删除后deployment会自动再创建一个名字为 mytomcat-xxxx 的pod.
  ```

  

#### 2.2 多副本

> 同一个服务创建多个副本

- 命令行

  ```sh
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  watch -n 1 kubectl get pod # 监控pod状态
  ```

- yaml

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: my-dep
    name: my-dep
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: my-dep
    template:
      metadata:
        labels:
          app: my-dep
      spec:
        containers:
        - image: nginx
          name: nginx
  ```

  

#### 2.3 自动扩容/缩容

> scale: 规模
>
> 流量高峰可以自动增加pod或者检查pod提高并发能力

- 扩容

  ```sh
  # 方式1
  kubectl scale deployment/my-dep --replicas=5 # 扩容名字为my-dep的deployment 副本为5
  # 方式2
  kubectl edit deployment/my-dep  # 修改deployment/my-dep配置文件中的 replicas 为5
  ```

  

- 缩容

  ```sh
  # 方式1
  kubectl scale deployment/my-dep --replicas=2 # 缩容名字为my-dep的deployment 副本为2
  # 方式2
  kubectl edit deployment/my-dep  # 修改deployment/my-dep配置文件中的 replicas 为2
  ```

*dashboard 中也可以同通过缩放完成扩缩容*

#### 2.4 故障转移

- 当pod发生停机/删除Pod/容器崩溃 等故障时
  - **自愈**: k8s尝试重启pod, 叫自愈
  - **故障转移**: 发生不可重启pod时, 比如机器断电等, k8s会在其他机器上拉起一份新的pod 叫故障转移

#### 2.5 滚动升级

>**滚动升级**（Rolling update）就是指每次更新部分Pod, 一个service可能有多个pod, 从v1升级到v2版本时, k8s首先会启动v2版本的pod, 每成功启动一个v2pod时, 停止一个v1pod, 将一部分流量指向v2pod. 直到所有pod更新成功
>
>- 优点: 不停机更新
>- 缺点: 会有数据版本不一致的情况

- 命令

  ```sh
  # 滚动升级
  kubectl set image deployment/my-dep nginx=nginx:1.16.1 --record  # --record 记录本次更新
  # 版本回退
  kubectl rollout status deployment/my-dep   
  ```

- 修改yaml文件

  ```sh
  kubectl edit deployment/my-dep
  ```

#### 2.6 版本回退

>根据滚动升级的版本进行滚动版本回退
>
>Deployment 是无状态的, 版本回退或者扩缩容自愈等之前的数据会丢失

- 命令

  ```sh
  #历史记录
  kubectl rollout history deployment/my-dep
  
  #查看某个历史详情
  kubectl rollout history deployment/my-dep --revision=2
  
  #回滚(回到上次)
  kubectl rollout undo deployment/my-dep
  
  #回滚(回到指定版本)
  kubectl rollout undo deployment/my-dep --to-revision=2
  ```

#### 2.7 小结

- 除了Deployment，k8s还有 `StatefulSet` 、`DaemonSet` 、`Job` 等 类型资源。我们都称为 `工作负载`。
- 有状态应用使用 `StatefulSet` 部署，无状态应用使用 `Deployment` 部署
- https://kubernetes.io/zh/docs/concepts/workloads/controllers/

## 3. Service

> - 服务: 将一组 pods 公开为网络服务的抽象方法。
> - service 会暴露集群端口给前端应用访问
> - 一个service 指向多个pod , 可以实现负载均衡的效果
> - 带有同一标签的pod创建时会自动加入到service
> - 删除service 时, 对应的deployment pod不会删除

<img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211225223813289.png" alt="image-20211225223813289" style="zoom: 50%;" />

- 命令暴露deployment

  ```sh
  #暴露Deploy
  kubectl expose deployment my-dep --port=8000 --target-port=80
  
  # 查看service
  kubectl get service
  
  #使用标签检索Pod
  kubectl get pod -l app=my-dep
  ```

- yaml 创建service

  ```yml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: my-dep
    name: my-dep
  spec:
    selector:
      app: my-dep
    ports:
    - port: 8000
      protocol: TCP
      targetPort: 80
  ```

#### 3.1 ClusterIP

> - service 默认的type是ClusterIP
>
> - ClusterIP类型只能在集群的内部使用
>
> - 创建的ClusterIp service访问:
>
>   - 在集群内部:
>
>     - **serviceIP:port** 访问, 	
>
>     - 也可以通过 **serviceName.serviceNameSpace.svc:port** 访问(此方式只能在其他pod内部访问)
>
>   - 不能在外部访问

- 命令创建 type为clusterip的service

  ```sh
  kubectl expose deployment my-dep --port=8000 --target-port=80
  #或者
  kubectl expose deployment my-dep --port=8000 --target-port=80 --type=ClusterIP
  ```

- yaml创建type为ClusterIP的service

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: my-dep
    name: my-dep
  spec:
    selector:
      app: my-dep
    ports:
    - port: 8000
      protocol: TCP
      targetPort: 80
    type: ClusterIP
    
  ```

  

#### 3.2 NodePort

> - NodePort 类型可以公共访问, 集群外部访问
>
> - NodePort范围在 30000-32767 之间, 会在每一台的集群机器上都创建一个相同的对外端口
>
> - 创建的NodePort 类型service 访问方式:
>   - 在集群内部:
>     - **serviceIP:port** 访问, 	
>     - 也可以通过 **serviceName.serviceNameSpace.svc:port** 访问(此方式只能在其他pod内部访问)
>   - 在集群外部: 使用任意集群内的公网IP加端口都可以访问到

- 命令创建NodePort 模式

  ```sh
  kubectl expose deployment my-dep --port=8000 --target-port=80 --type=NodePort
  ```

- yaml创建NodePort 模式

  ```yml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: my-dep
    name: my-dep
  spec:
    ports:
    - port: 8000
      protocol: TCP
      targetPort: 80
    selector:
      app: my-dep
    type: NodePort
  
  ```

  

## 4. Ingress

> Ingress(入口): Service 的统一网关入口 
>
> - 底层是nginx 做的反向代理
> - 官网地址：https://kubernetes.github.io/ingress-nginx/

<img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211225233956736.png" alt="image-20211225233956736" style="zoom: 50%;" />

- k8s网络模型

<img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211226132512138.png" style="zoom:50%;" />

#### 4.1 安装Ingress

```sh
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml

#修改镜像
vi deploy.yaml
#将image的值改为如下值：
registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0

# 检查安装的结果
kubectl get pod,svc -n ingress-nginx

# 最后别忘记把svc暴露的端口要放行
```

![image-20211225235307284](https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211225235307284.png)

- 修改后的deploy.yaml

  ```yaml
  apiVersion: v1
  kind: Namespace
  metadata:
    name: ingress-nginx
    labels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
  
  ---
  # Source: ingress-nginx/templates/controller-serviceaccount.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx
    namespace: ingress-nginx
  automountServiceAccountToken: true
  ---
  # Source: ingress-nginx/templates/controller-configmap.yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx-controller
    namespace: ingress-nginx
  data:
  ---
  # Source: ingress-nginx/templates/clusterrole.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
    name: ingress-nginx
  rules:
    - apiGroups:
        - ''
      resources:
        - configmaps
        - endpoints
        - nodes
        - pods
        - secrets
      verbs:
        - list
        - watch
    - apiGroups:
        - ''
      resources:
        - nodes
      verbs:
        - get
    - apiGroups:
        - ''
      resources:
        - services
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - extensions
        - networking.k8s.io   # k8s 1.14+
      resources:
        - ingresses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ''
      resources:
        - events
      verbs:
        - create
        - patch
    - apiGroups:
        - extensions
        - networking.k8s.io   # k8s 1.14+
      resources:
        - ingresses/status
      verbs:
        - update
    - apiGroups:
        - networking.k8s.io   # k8s 1.14+
      resources:
        - ingressclasses
      verbs:
        - get
        - list
        - watch
  ---
  # Source: ingress-nginx/templates/clusterrolebinding.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
    name: ingress-nginx
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: ingress-nginx
  subjects:
    - kind: ServiceAccount
      name: ingress-nginx
      namespace: ingress-nginx
  ---
  # Source: ingress-nginx/templates/controller-role.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx
    namespace: ingress-nginx
  rules:
    - apiGroups:
        - ''
      resources:
        - namespaces
      verbs:
        - get
    - apiGroups:
        - ''
      resources:
        - configmaps
        - pods
        - secrets
        - endpoints
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ''
      resources:
        - services
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - extensions
        - networking.k8s.io   # k8s 1.14+
      resources:
        - ingresses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - extensions
        - networking.k8s.io   # k8s 1.14+
      resources:
        - ingresses/status
      verbs:
        - update
    - apiGroups:
        - networking.k8s.io   # k8s 1.14+
      resources:
        - ingressclasses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ''
      resources:
        - configmaps
      resourceNames:
        - ingress-controller-leader-nginx
      verbs:
        - get
        - update
    - apiGroups:
        - ''
      resources:
        - configmaps
      verbs:
        - create
    - apiGroups:
        - ''
      resources:
        - events
      verbs:
        - create
        - patch
  ---
  # Source: ingress-nginx/templates/controller-rolebinding.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx
    namespace: ingress-nginx
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: ingress-nginx
  subjects:
    - kind: ServiceAccount
      name: ingress-nginx
      namespace: ingress-nginx
  ---
  # Source: ingress-nginx/templates/controller-service-webhook.yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx-controller-admission
    namespace: ingress-nginx
  spec:
    type: ClusterIP
    ports:
      - name: https-webhook
        port: 443
        targetPort: webhook
    selector:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/component: controller
  ---
  # Source: ingress-nginx/templates/controller-service.yaml
  apiVersion: v1
  kind: Service
  metadata:
    annotations:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx-controller
    namespace: ingress-nginx
  spec:
    type: NodePort
    ports:
      - name: http
        port: 80
        protocol: TCP
        targetPort: http
      - name: https
        port: 443
        protocol: TCP
        targetPort: https
    selector:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/component: controller
  ---
  # Source: ingress-nginx/templates/controller-deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: controller
    name: ingress-nginx-controller
    namespace: ingress-nginx
  spec:
    selector:
      matchLabels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/component: controller
    revisionHistoryLimit: 10
    minReadySeconds: 0
    template:
      metadata:
        labels:
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/component: controller
      spec:
        dnsPolicy: ClusterFirst
        containers:
          - name: controller
            image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0
            imagePullPolicy: IfNotPresent
            lifecycle:
              preStop:
                exec:
                  command:
                    - /wait-shutdown
            args:
              - /nginx-ingress-controller
              - --election-id=ingress-controller-leader
              - --ingress-class=nginx
              - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
              - --validating-webhook=:8443
              - --validating-webhook-certificate=/usr/local/certificates/cert
              - --validating-webhook-key=/usr/local/certificates/key
            securityContext:
              capabilities:
                drop:
                  - ALL
                add:
                  - NET_BIND_SERVICE
              runAsUser: 101
              allowPrivilegeEscalation: true
            env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: LD_PRELOAD
                value: /usr/local/lib/libmimalloc.so
            livenessProbe:
              failureThreshold: 5
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            readinessProbe:
              failureThreshold: 3
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              successThreshold: 1
              timeoutSeconds: 1
            ports:
              - name: http
                containerPort: 80
                protocol: TCP
              - name: https
                containerPort: 443
                protocol: TCP
              - name: webhook
                containerPort: 8443
                protocol: TCP
            volumeMounts:
              - name: webhook-cert
                mountPath: /usr/local/certificates/
                readOnly: true
            resources:
              requests:
                cpu: 100m
                memory: 90Mi
        nodeSelector:
          kubernetes.io/os: linux
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        volumes:
          - name: webhook-cert
            secret:
              secretName: ingress-nginx-admission
  ---
  # Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml
  # before changing this value, check the required kubernetes version
  # https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites
  apiVersion: admissionregistration.k8s.io/v1
  kind: ValidatingWebhookConfiguration
  metadata:
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
    name: ingress-nginx-admission
  webhooks:
    - name: validate.nginx.ingress.kubernetes.io
      matchPolicy: Equivalent
      rules:
        - apiGroups:
            - networking.k8s.io
          apiVersions:
            - v1beta1
          operations:
            - CREATE
            - UPDATE
          resources:
            - ingresses
      failurePolicy: Fail
      sideEffects: None
      admissionReviewVersions:
        - v1
        - v1beta1
      clientConfig:
        service:
          namespace: ingress-nginx
          name: ingress-nginx-controller-admission
          path: /networking/v1beta1/ingresses
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: ingress-nginx-admission
    annotations:
      helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
    namespace: ingress-nginx
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: ingress-nginx-admission
    annotations:
      helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
  rules:
    - apiGroups:
        - admissionregistration.k8s.io
      resources:
        - validatingwebhookconfigurations
      verbs:
        - get
        - update
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: ingress-nginx-admission
    annotations:
      helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: ingress-nginx-admission
  subjects:
    - kind: ServiceAccount
      name: ingress-nginx-admission
      namespace: ingress-nginx
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    name: ingress-nginx-admission
    annotations:
      helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
    namespace: ingress-nginx
  rules:
    - apiGroups:
        - ''
      resources:
        - secrets
      verbs:
        - get
        - create
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: ingress-nginx-admission
    annotations:
      helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
    namespace: ingress-nginx
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: ingress-nginx-admission
  subjects:
    - kind: ServiceAccount
      name: ingress-nginx-admission
      namespace: ingress-nginx
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: ingress-nginx-admission-create
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
    namespace: ingress-nginx
  spec:
    template:
      metadata:
        name: ingress-nginx-admission-create
        labels:
          helm.sh/chart: ingress-nginx-3.33.0
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/version: 0.47.0
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/component: admission-webhook
      spec:
        containers:
          - name: create
            image: docker.io/jettech/kube-webhook-certgen:v1.5.1
            imagePullPolicy: IfNotPresent
            args:
              - create
              - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
              - --namespace=$(POD_NAMESPACE)
              - --secret-name=ingress-nginx-admission
            env:
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
        restartPolicy: OnFailure
        serviceAccountName: ingress-nginx-admission
        securityContext:
          runAsNonRoot: true
          runAsUser: 2000
  ---
  # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: ingress-nginx-admission-patch
    annotations:
      helm.sh/hook: post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    labels:
      helm.sh/chart: ingress-nginx-3.33.0
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/version: 0.47.0
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/component: admission-webhook
    namespace: ingress-nginx
  spec:
    template:
      metadata:
        name: ingress-nginx-admission-patch
        labels:
          helm.sh/chart: ingress-nginx-3.33.0
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/version: 0.47.0
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/component: admission-webhook
      spec:
        containers:
          - name: patch
            image: docker.io/jettech/kube-webhook-certgen:v1.5.1
            imagePullPolicy: IfNotPresent
            args:
              - patch
              - --webhook-name=ingress-nginx-admission
              - --namespace=$(POD_NAMESPACE)
              - --patch-mutating=false
              - --secret-name=ingress-nginx-admission
              - --patch-failure-policy=Fail
            env:
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
        restartPolicy: OnFailure
        serviceAccountName: ingress-nginx-admission
        securityContext:
          runAsNonRoot: true
          runAsUser: 2000
  ```

#### 4.2 使用

> - 官网地址：https://kubernetes.github.io/ingress-nginx/

- demo `a b c`的测试环境

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: hello-server
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: hello-server
    template:
      metadata:
        labels:
          app: hello-server
      spec:
        containers:
        - name: hello-server
          image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server
          ports:
          - containerPort: 9000
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: nginx-demo
    name: nginx-demo
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx-demo
    template:
      metadata:
        labels:
          app: nginx-demo
      spec:
        containers:
        - image: nginx
          name: nginx
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-demo
    name: nginx-demo
  spec:
    selector:
      app: nginx-demo
    ports:
    - port: 8000
      protocol: TCP
      targetPort: 80
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: hello-server
    name: hello-server
  spec:
    selector:
      app: hello-server
    ports:
    - port: 8000
      protocol: TCP
      targetPort: 9000
  ```

  

##### a. 域名访问

<img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211226003231753.png" alt="image-20211226003231753" style="zoom:50%;" />

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress  
metadata:
  name: ingress-host-bar
spec:
  ingressClassName: nginx
  rules:
  - host: "hello.atguigu.com"
    http:
      paths:
      - pathType: Prefix  # 前缀模式
        path: "/"
        backend:
          service:
            name: hello-server
            port:
              number: 8000
  - host: "demo.atguigu.com"
    http:
      paths:
      - pathType: Prefix
        path: "/nginx"  # 把请求会转给下面的服务，下面的服务一定要能处理这个路径，不能处理就是404
        backend:
          service:
            name: nginx-demo  ## java，比如使用路径重写，去掉前缀nginx
            port:
              number: 8000
```

- 本机配置域名映射
- 访问
  - http://hello.atguigu.com:30948/
  - http://demo.atguigu.com:30948/nginx  */nginx 路径访问api时会被携带*

##### b. 路径重写

> 处理ingress请求路径, 保证访问到service中的路径

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress  
metadata:
  annotations: 
    nginx.ingress.kubernetes.io/rewrite-target: /$2  # 路径重写
  name: ingress-host-bar
spec:
  ingressClassName: nginx
  rules:
  - host: "hello.atguigu.com"
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: hello-server
            port:
              number: 8000
  - host: "demo.atguigu.com"
    http:
      paths:
      - pathType: Prefix
        path: "/nginx(/|$)(.*)"  # 把请求会转给下面的服务，下面的服务一定要能处理这个路径，不能处理就是404, ingress 访问service时会去掉nginx, 匹配(.*)内容
        backend:
          service:
            name: nginx-demo  ## java，比如使用路径重写，去掉前缀 nginx
            port:
              number: 8000
```



##### c. 流量限制

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-limit-rate
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "1" # 限流, 每秒1个
spec:
  ingressClassName: nginx
  rules:
  - host: "haha.atguigu.com"
    http:
      paths:
      - pathType: Exact  # 精确模式
        path: "/"
        backend:
          service:
            name: nginx-demo
            port:
              number: 8000
```



## 5. 存储抽象

> - 在docker中, 为了数据安全, 会将容器的数据挂载到宿主机的磁盘上
>   - 问题: 当集群中有的容器挂掉之后, 集群在其他主机上重新拉起的容器不能访问上一台宿主机磁盘上的数据.
> - 在k8s集群中, 会将存储层抽象出来, 所有的pod数据全部挂载到存储层, k8s自动管理存储层
>   - 存储层技术: NFS(网络存储),Glusterfs, CephFS

- 架构图

  <img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211226134009002.png" alt="image-20211226134009002" style="zoom: 33%;" />

- NFS网络存储系统会在某一台节点上(NFS的主节点)创建一个`/nfs/data`数据目录,在其他节点(NFS从节点)上创建`/bak/bada`, 任意一个节点对文件修改都会在所有节点上同步, 实现数据一致

### 5.1 安装存储系统

- 存储层技术: NFS(网络存储),Glusterfs, CephFS, 这里使用NFS

1. 所有节点 安装nfs

   ```sh
   yum install -y nfs-utils
   ```

2. 主节点

   ```sh
   #nfs主节点
   echo "/nfs/data/ *(insecure,rw,sync,no_root_squash)" > /etc/exports
   
   mkdir -p /nfs/data
   systemctl enable rpcbind --now
   systemctl enable nfs-server --now
   #配置生效
   exportfs -r
   ```

3. 从节点

   ```sh
   showmount -e 10.120.11.35
   
   #执行以下命令挂载 nfs 服务器上的共享目录到本机路径 /root/nfsmount
   mkdir -p /nfs/data
   
   mount -t nfs 10.120.11.35:/nfs/data /nfs/data
   # 写入一个测试文件
   echo "hello nfs server" > /nfs/data/test.txt
   ```

### 5.2 k8s原生方式数据挂载

> - 问题:
>   - 在pod中修改了文件, nfs数据卷中的数据不会更新, 反之也是如此
>   - 需要手动创建nfs数据卷文件夹
> - 推荐使用PV|VC

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-pv-demo
  name: nginx-pv-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-pv-demo
  template:
    metadata:
      labels:
        app: nginx-pv-demo
    spec:
      containers:
      - image: nginx
        name: nginx
        volumeMounts: # pod数据卷
        - name: html  
          mountPath: /usr/share/nginx/html
      volumes:   # nfs 数据卷
        - name: html
          nfs:
            server: 10.120.11.35
            path: /nfs/data/nginx-pv
```

### 5.3 PV&PVC

> *PV：持久卷（Persistent Volume），将应用需要持久化的数据保存到指定位置*       **场地**
>
> PVC：持久卷申明（**Persistent Volume Claim**），申明需要使用的持久卷规格,     **场地申请书**
>
> PV&PVC配合使用: pod中修改了数据卷中的数据, pv池中的数据也会更新
>
> PV 可以静态供应也可以动态供应: 
>
> - 静态供应: 一次性分配空间
> - 动态供应: 动态分配空间
>
> PV&PVC挂载目录, ConfigMap挂载配置文件

<img src="/Users/liusaisai/Library/Application Support/typora-user-images/image-20211226141747387.png" alt="image-20211226141747387" style="zoom: 33%;" />

##### a. 创建PV池

- 创建数据卷(静态供应)

  ```sh
  #nfs主节点
  mkdir -p /nfs/data/01
  mkdir -p /nfs/data/02
  mkdir -p /nfs/data/03
  ```

- 创建PV

  ```yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv01-10m
  spec:
    capacity:
      storage: 10M
    accessModes:
      - ReadWriteMany
    storageClassName: nfs  # 存储名称,分组, 可以随便写, 上下要统一
    nfs:
      path: /nfs/data/01
      server: 10.120.11.35
  ---
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv02-1gi
  spec:
    capacity:
      storage: 1Gi
    accessModes:
      - ReadWriteMany
    storageClassName: nfs
    nfs:
      path: /nfs/data/02
      server: 10.120.11.35
  ---
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv03-3gi
  spec:
    capacity:
      storage: 3Gi
    accessModes:
      - ReadWriteMany
    storageClassName: nfs
    nfs:
      path: /nfs/data/03
      server: 10.120.11.35
  ```

  

##### b. PVC创建与绑定

- 创建PVC

  ```yaml
  kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: nginx-pvc
  spec:
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 200Mi      # 申请200M空间, hu
    storageClassName: nfs   # 指定分组, pv存储名称
  ```

- 创建Pod绑定PVC

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: nginx-deploy-pvc
    name: nginx-deploy-pvc
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx-deploy-pvc
    template:
      metadata:
        labels:
          app: nginx-deploy-pvc
      spec:
        containers:
        - image: nginx
          name: nginx
          volumeMounts:
          - name: html
            mountPath: /usr/share/nginx/html
        volumes:
          - name: html
            persistentVolumeClaim:
              claimName: nginx-pvc   # 指定pvc申请书名称
  ```

  

### 5.4 ConfigMap

> - PV&PVC挂载目录, ConfigMap挂载配置文件
> - 抽取应用配置，并且可以自动更新

- 下面以redis示例, 使用ConfigMap

  <img src="https://raw.githubusercontent.com/daniuEvan/pictrues/main/Typora/image-20211226150237625.png" alt="image-20211226150237625" style="zoom: 50%;" />

  1. 创建redis.conf

     ```sh
     appendonly yes
     ```

  2. 把redis.conf 配置文件创建为配置集

     ```sh
     # 从conf文件创建配置，redis 保存到k8s的etcd；
     kubectl create cm redis-conf --from-file=redis.conf
     ```

     ```yaml
     # yaml创建configmap
     apiVersion: v1
     data:    #data是所有真正的数据，key：默认是文件名   value：配置文件的内容
       redis.conf: |
         appendonly yes
     kind: ConfigMap
     metadata:
       name: redis-conf
       namespace: default
     ```

  3. 创建pod并指定ConfigMap

     ```yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: redis
     spec:
       containers:
       - name: redis
         image: redis
         command:
           - redis-server
           - "/redis-master/redis.conf"  # 指的是redis容器内部的位置
         ports:
         - containerPort: 6379
         volumeMounts:
         - mountPath: /data      # configmap 挂载路径
           name: data
         - mountPath: /redis-master    # redis pod内部路径
           name: config
       volumes:
         - name: data
           emptyDir: {}   # k8s会自动分配目录(configmap 挂载路径)
         - name: config
           configMap:
             name: redis-conf   # 名称为 redis-conf 的 configmap
             items:  # 名称为 redis-conf 的 configmap 下的所有配置集 
             - key: redis.conf   # 获取名称为 redis-conf 的 configmap  下key为redis.conf的value
               path: redis.conf  # redis pod 内部文件名
     ```

  4. 检查默认配置

     ```yaml
     # kubectl get configmap redis-conf -o yaml
     apiVersion: v1
     data:
       redis.conf: |
         appendonly yes
     kind: ConfigMap
     ```

  5. 修改configmap, redis pod中的配置文件也会自动更新

     `kubectl edit configmap redis-config`

     ```yaml
     apiVersion: v1
     kind: ConfigMap
     metadata:
       name: example-redis-config
     data:
       redis-config: |
         maxmemory 2mb
         maxmemory-policy allkeys-lru 
     ```

  6. 检查配置是否更新

     ```sh
     kubectl exec -it redis -- redis-cli
     
     127.0.0.1:6379> CONFIG GET maxmemory
     127.0.0.1:6379> CONFIG GET maxmemory-policy
     ```

  > 修改了CM。Pod里面的配置文件会跟着变, 但是redis 中的**配置值未更改，因为需要重新启动 Pod 才能从关联的 ConfigMap 中获取更新的值。**
  >
  > **原因：我们的Pod部署的中间件自己本身没有热更新能力**

### 5.5 Secret

> - Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)的定义或者 [容器镜像](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-image) 中来说更加安全和灵活。
> - docker仓库有些私有镜像, 需要登录或者提供镜像秘钥, pod在拉取镜像时指定秘钥信息即可

```sh
kubectl create secret docker-registry leifengyang-docker \
--docker-username=leifengyang \
--docker-password=Lfy123456 \
--docker-email=534096094@qq.com

## 命令格式
kubectl create secret docker-registry regcred \
  --docker-server=<你的镜像仓库服务器> \
  --docker-username=<你的用户名> \
  --docker-password=<你的密码> \
  --docker-email=<你的邮箱地址>
```



```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-nginx
spec:
  containers:
  - name: private-nginx
    image: leifengyang/guignginx:v1.0
  imagePullSecrets:
  - name: leifengyang-docker   # 指定秘钥
```

